{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install datasets\n",
    "# !pip install trl\n",
    "# !pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = '' # use your HF token\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "# model_name = \"cike-dev/GemmaBullyClassifier-e\"\n",
    "# model_name = \"cike-dev/Gemma_3_Toxic_Classifier\" # works fine\n",
    "# model_name = \"cike-dev/GemmaToxicClassifier\"\n",
    "# # model_name = \"cike-dev/GemmaToxicity\"\n",
    "\n",
    "# Auto GPU + dtype handling\n",
    "device_map = \"auto\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Optional: bitsandbytes config (like unslothâ€™s flags)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb_config,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "# Patch chat template (same as unsloth)\n",
    "tokenizer.chat_template = \"\"\"{{ bos_token }}{% for message in messages %}\n",
    "<start_of_turn>{{message['role']}}\n",
    "{{message['content']}}<end_of_turn>\n",
    "{% endfor %}{% if add_generation_prompt %}<start_of_turn>model\n",
    "{% endif %}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n",
      "cuda:0\n",
      "{{ bos_token }}{% for message in messages %}\n",
      "<start_of_turn>{{message['role']}}\n",
      "{{message['content']}}<end_of_turn>\n",
      "{% endfor %}{% if add_generation_prompt %}<start_of_turn>model\n",
      "{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_position_embeddings)\n",
    "print(model.device)\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing classification ability of the model before finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While the text is described as \"toxic,\" it's not actually \"toxic.\" It's purely descriptive and doesn't contain any actual life experiences.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\t# {\"role\": \"system\",\n",
    "\t# \"content\": \"Classify the text as 'toxic' or 'normal'. Output only one word.\"},\n",
    "    {\"role\": \"user\", \n",
    "\t\"content\": \"Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'you either stand with your people or they will stand over your dead body .'\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "# )\n",
    "# inputs\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_set = load_dataset(\"cike-dev/en_toxic_set\", split='test')\n",
    "\n",
    "\n",
    "# for i in raw_set.select(range(2)):\n",
    "#     print(i['cleaned_text'])\n",
    "\n",
    "# raw_set['cleaned_text'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot testing\n",
    "\n",
    "using prompt engineering to steer the model's tone can produce unpredictable and may not always align with the expected persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\"}, {'role': 'user', 'content': 'the fundamental basis of free speech is the marketplace of ideas.'}, {'role': 'assistant', 'content': 'normal'}, {'role': 'user', 'content': 'i do not silence anybody that kike shit not only do i not silence people i give them a voice on my open phones talk show the fact that there are so many people deserving of my scorn is a separate topic entirely'}, {'role': 'assistant', 'content': 'toxic'}, {'role': 'user', 'content': 'fire her nasty ass.'}, {'role': 'assistant', 'content': 'I am a large language model this response is a response based on your previous response. I am ready to receive your response and then to provide helpful information and assistance in a way that is respectful and appropriate.'}]\n",
      "--------------------------------------------------------------------------------\n",
      "I am a large language model this response is a response based on your previous response. I am ready to receive your response and then to provide helpful information and assistance in a way that is respectful and appropriate.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "raw_set = load_dataset(\"cike-dev/en_toxic_set\", split='test')\n",
    "\n",
    "\n",
    "message = [\n",
    "    # give persona\n",
    "    {\"role\": \"system\", \"content\": \"You are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\"},\n",
    "]\n",
    "\n",
    "\n",
    "# few shot prompt\n",
    "for item in raw_set.select(range(2)):\n",
    "  message.append(\n",
    "      {\"role\": \"user\", \"content\": item[\"cleaned_text\"]}\n",
    "  )\n",
    "  message.append(\n",
    "      {\"role\": \"assistant\", \"content\": item[\"label\"]}\n",
    "  )\n",
    "\n",
    "# actual question\n",
    "message.append(\n",
    "    {\"role\": \"user\", \"content\": raw_set[\"cleaned_text\"][100]}\n",
    ")\n",
    "\n",
    "outputs = pipe(message, max_new_tokens=256, disable_compile=True)\n",
    "print(outputs[0]['generated_text'])\n",
    "print(\"-\"*80)\n",
    "print(outputs[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   4335,   1891,    506,   1816,    618,\n",
       "            756,  72401, 236789,    653,    756,   7382,   6748,  16887,   1186,\n",
       "            886,   3658, 236761,   5590, 236787,    756,   7624,   3477,   1975,\n",
       "            607,    822,   1331,    653,    901,    795,   1975,   1024,    822,\n",
       "           6582,   2742, 209442,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.4: Fast Gemma3_Text patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.278 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    max_seq_length=256,\n",
    "    dtype=torch.bfloat16,  # auto detect bf16/fp16\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = True,  # full fine-tuning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cleaned_text': 'methinks grahamnasty has seen the writing on the wall after the mark sanford loss...', 'label': 'normal'}\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 2. Load Dataset\n",
    "# ===========================\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cike-dev/en_toxic_set\")\n",
    "# train split\n",
    "train_dataset = dataset[\"train\"]\n",
    "# train_dataset = train_dataset.select(range(10000))\n",
    "\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "eval_dataset = eval_dataset.select(range(4000))\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "print(train_dataset[3000])\n",
    "\n",
    "\n",
    "# Obtain the correct chat template\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma3\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"You are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\",\n",
       "  'role': 'system'},\n",
       " {'content': \"Classify this text: 'the definition of traitor is then the issue  say you did business with a jew, but nothing special, does that make you a traitor, etc.'\",\n",
       "  'role': 'user'},\n",
       " {'content': 'normal', 'role': 'assistant'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"You are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\",\n",
       "  'role': 'system'},\n",
       " {'content': \"Classify this text: 'same in the uk election postal vote is exploited by muslims in particular female muslims give their vote to whomever the imam says if it a muslim candidate they just bring voters in register from a muslim house and job done'\",\n",
       "  'role': 'user'},\n",
       " {'content': 'toxic', 'role': 'assistant'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_chatml(example):\n",
    "    system_message = \"You are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\"\n",
    "    user_prompt = f\"Classify this text: '{example['cleaned_text']}'\"\n",
    "    \n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"label\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "train_set = train_dataset.map(convert_to_chatml)\n",
    "eval_set = eval_dataset.map(convert_to_chatml)\n",
    "\n",
    "display(train_set['conversations'][1])\n",
    "print()\n",
    "display(eval_set['conversations'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start_of_turn>user\\nYou are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\\n\\nClassify this text: 'the definition of traitor is then the issue  say you did business with a jew, but nothing special, does that make you a traitor, etc.'<end_of_turn>\\n<start_of_turn>model\\nnormal<end_of_turn>\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<start_of_turn>user\\nYou are a binary classifier for cyberbullying speech. Respond with one word: 'toxic' or 'normal'.\\n\\nClassify this text: 'same in the uk election postal vote is exploited by muslims in particular female muslims give their vote to whomever the imam says if it a muslim candidate they just bring voters in register from a muslim house and job done'<end_of_turn>\\n<start_of_turn>model\\ntoxic<end_of_turn>\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   msgs = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(msg, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for msg in msgs]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "# Apply the function to the datasets\n",
    "\n",
    "train_set = train_set.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "eval_set = eval_set.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "display(train_set['text'][1])\n",
    "print()\n",
    "display(eval_set['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========================\n",
    "# 4. Training Config\n",
    "# ===========================\n",
    "import os\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "saved_model = \"./Gemma3ToxicTextClassifier\"\n",
    "\n",
    "os.makedirs(saved_model, exist_ok=True)\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    dataset_text_field=\"text\",      # dataset column with input text\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    max_length=256,\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=500,                      # avoid logging every step\n",
    "    optim=\"adamw_torch_fused\",              # fused optimizer\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"constant\",           # conctant, linear, or cosine\n",
    "    seed=2025,\n",
    "    output_dir=saved_model,\n",
    "    report_to=\"none\",\n",
    "    bf16=torch_dtype == torch.bfloat16,                              # use bf16 if GPU supports, else fp16=True\n",
    "    fp16=torch_dtype == torch.float16,\n",
    "    # metric_for_best_model=\"eval_loss\",    # or f1_score, is computed correctly\n",
    "    # greater_is_better=False,\n",
    "    # load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # Template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=eval_set,\n",
    "    args=sft_args,\n",
    "    # formatting_func = formatting_func,**kwargs\n",
    ")\n",
    "\n",
    "torch_dtype == torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 76,259 | Num Epochs = 5 | Total steps = 47,665\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 268,098,176 of 268,098,176 (100.00% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47665' max='47665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47665/47665 2:58:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.688900</td>\n",
       "      <td>1.651730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.612700</td>\n",
       "      <td>1.643584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.573500</td>\n",
       "      <td>1.650423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.522100</td>\n",
       "      <td>1.658994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.504300</td>\n",
       "      <td>1.669838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 5. Train\n",
    "# ===========================\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=47665, training_loss=1.5785336285463776, metrics={'train_runtime': 10735.0355, 'train_samples_per_second': 35.519, 'train_steps_per_second': 4.44, 'total_flos': 3.016052380955443e+16, 'train_loss': 1.5785336285463776, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Gemma3ToxicTextClassifier/tokenizer_config.json',\n",
       " './Gemma3ToxicTextClassifier/special_tokens_map.json',\n",
       " './Gemma3ToxicTextClassifier/chat_template.jinja',\n",
       " './Gemma3ToxicTextClassifier/tokenizer.model',\n",
       " './Gemma3ToxicTextClassifier/added_tokens.json',\n",
       " './Gemma3ToxicTextClassifier/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5405651759444a94258ffe42e3e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7938e08c02e4540a306b8702c9d91fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d533b33def4ba09ce26248c3642e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ToxicTextClassifier/tokenizer.model:  97%|#########7| 4.55MB / 4.69MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc553cf44134b5883db11674c939a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...xicTextClassifier/model.safetensors:   0%|          |  612kB /  536MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519eea80851a41199a09dfeb5ecc5f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...3ToxicTextClassifier/tokenizer.json:  97%|#########7| 32.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4005b0412361489da98a34e4331ff4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...xicTextClassifier/training_args.bin: 100%|##########| 6.35kB / 6.35kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/cike-dev/Gemma3ToxicTextClassifier/commit/c75d7b607c8a58d344732058b3b9c0661fc3d815', commit_message='cike-dev/Gemma3ToxicTextClassifier', commit_description='', oid='c75d7b607c8a58d344732058b3b9c0661fc3d815', pr_url=None, repo_url=RepoUrl('https://huggingface.co/cike-dev/Gemma3ToxicTextClassifier', endpoint='https://huggingface.co', repo_type='model', repo_id='cike-dev/Gemma3ToxicTextClassifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"cike-dev/Gemma3ToxicTextClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UnslothSFTTrainer' object has no attribute '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Access the log history\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m log_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241m.\u001b[39mlog_history\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract training / validation loss\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m log_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnslothSFTTrainer' object has no attribute '_state'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the log history\n",
    "log_history = trainer._state.log_history\n",
    "\n",
    "# Extract training / validation loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===========================\n",
    "# # 6. Batched Evaluation\n",
    "# # ===========================\n",
    "# def batched_eval(trainer, dataset, batch_size=16, max_new_tokens=8):\n",
    "#     model = trainer.model\n",
    "#     tok = trainer.tokenizer\n",
    "#     device = next(model.parameters()).device\n",
    "#     model.eval()\n",
    "\n",
    "#     texts = [ex[\"cleaned_text\"] for ex in dataset]\n",
    "#     labels = [1 if ex[\"label\"].lower()==\"toxic\" else 0 for ex in dataset]\n",
    "\n",
    "#     preds = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[i: i+batch_size]\n",
    "#         enc = tok(batch_texts, return_tensors=\"pt\", padding=True,\n",
    "#                   truncation=True, max_length=128)\n",
    "#         enc = {k:v.to(device) for k,v in enc.items()}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **enc,\n",
    "#                 max_new_tokens=max_new_tokens,\n",
    "#                 do_sample=False,\n",
    "#                 temperature=0.0,\n",
    "#                 pad_token_id=tok.pad_token_id,\n",
    "#                 eos_token_id=tok.eos_token_id,\n",
    "#             )\n",
    "\n",
    "#         input_lens = (enc[\"input_ids\"] != tok.pad_token_id).sum(dim=1).tolist()\n",
    "#         for out_ids, in_len in zip(outputs.cpu().tolist(), input_lens):\n",
    "#             gen_ids = out_ids[in_len:] if len(out_ids)>in_len else []\n",
    "#             decoded_gen = tok.decode(gen_ids, skip_special_tokens=True).strip().lower()\n",
    "\n",
    "#             if \"toxic\" in decoded_gen:\n",
    "#                 preds.append(1)\n",
    "#             elif \"normal\" in decoded_gen:\n",
    "#                 preds.append(0)\n",
    "#             else:\n",
    "#                 # fallback: last token\n",
    "#                 toks = decoded_gen.split()\n",
    "#                 if toks and toks[-1].startswith(\"toxic\"): preds.append(1)\n",
    "#                 elif toks and toks[-1].startswith(\"normal\"): preds.append(0)\n",
    "#                 else: preds.append(0)  # default to normal\n",
    "\n",
    "#     return preds, labels\n",
    "\n",
    "# # Run evaluation\n",
    "# preds, labels = batched_eval(trainer, eval_dataset, batch_size=16)\n",
    "\n",
    "# acc = accuracy_score(labels, preds)\n",
    "# f1 = f1_score(labels, preds)\n",
    "# print(f\"\\nValidation Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "# print(classification_report(labels, preds, target_names=[\"normal\",\"toxic\"], digits=4))\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(labels, preds)\n",
    "# plt.figure(figsize=(5,5))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", square=True,\n",
    "#             xticklabels=[\"normal\",\"toxic\"], yticklabels=[\"normal\",\"toxic\"],\n",
    "#             annot_kws={\"size\":14})\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.title(\"Confusion Matrix - Validation Set\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(saved_model,\"confusion_matrix.png\"), dpi=300)\n",
    "# plt.show()\n",
    "# plt.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcike-dev/Gemma3ToxicTextClassifier-bkp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcike-dev/Gemma3ToxicTextClassifier-bkp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"cike-dev/Gemma3ToxicTextClassifier-bkp\")\n",
    "tokenizer.push_to_hub(\"cike-dev/Gemma3ToxicTextClassifier-bkp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
