{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if needed\n",
    "\n",
    "# !pip install --no-deps --upgrade -q seaborn\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import unsloth\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from unsloth import FastModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path: ./Gemma3ToxicTextClassifier\n",
      "Data Path: cike-dev/en_toxic_set\n",
      "Output Directory: unsloth-gemma3-full-ft\n"
     ]
    }
   ],
   "source": [
    "## Cell 2: Configuration\n",
    "# Configuration - Modify these paths as needed\n",
    "MODEL_PATH = \"./Gemma3ToxicTextClassifier\"\n",
    "DATA_PATH = \"cike-dev/en_toxic_set\"\n",
    "OUTPUT_DIR = \"unsloth-gemma3-full-ft\"\n",
    "MAX_SEQ_LENGTH = 256\n",
    "CHAT_TEMPLATE = \"gemma3\"\n",
    "\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelEvaluator class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"A class for evaluating fine-tuned language models on classification tasks.\"\"\"\n",
    "    \n",
    "    # def __init__(self, model_path: str, max_seq_length: int = 128, chat_template: str = \"gemma3\"):\n",
    "    #     \"\"\"Initialize the evaluator with a model.\"\"\"\n",
    "    #     self.model_path = model_path\n",
    "    #     self.model, self.tokenizer = self._load_model(model_path, max_seq_length, chat_template)\n",
    "\n",
    "\n",
    "    def __init__(self, model_path: str, max_seq_length: int = 512, chat_template: str = \"gemma3\", device: Optional[str] = None):\n",
    "        \"\"\"Initialize the evaluator with a model.\"\"\"\n",
    "        self.device = torch.device(device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.model_path = model_path\n",
    "        self.model, self.tokenizer = self._load_model(model_path, max_seq_length, chat_template)\n",
    "        # # move model to device and set eval\n",
    "        # self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    \n",
    "    # def _load_model(self, model_path: str, max_seq_length: int, chat_template: str):\n",
    "    #     \"\"\"Load the model and tokenizer.\"\"\"\n",
    "    #     print(f\"Loading model from {model_path}...\")\n",
    "    #     model, tokenizer = FastModel.from_pretrained(\n",
    "    #         model_name=model_path,\n",
    "    #         max_seq_length=max_seq_length,\n",
    "    #     )\n",
    "        \n",
    "    #     tokenizer = get_chat_template(tokenizer, chat_template=chat_template)\n",
    "    #     print(\"Model loaded successfully!\")\n",
    "    #     return model, tokenizer\n",
    "\n",
    "    def _load_model(self, model_path: str, max_seq_length: int, chat_template: str):\n",
    "        \"\"\"Load the model and tokenizer.\"\"\"\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        # NOTE: change FastModel to your actual loader (this code assumes it returns (model, tokenizer))\n",
    "        model, tokenizer = FastModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "        )\n",
    "        tokenizer = get_chat_template(tokenizer, chat_template=chat_template)\n",
    "\n",
    "        # Make sure decoder models use left padding if needed\n",
    "        try:\n",
    "            tokenizer.padding_side = \"left\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def _format_input(self, conversations: List[Dict]) -> str:\n",
    "        \"\"\"Format input using chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": conversations[0][\"content\"]},\n",
    "            {\"role\": \"user\", \"content\": conversations[1][\"content\"]}\n",
    "        ]\n",
    "        \n",
    "        input_text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ).removeprefix('<bos>')\n",
    "        \n",
    "        return input_text\n",
    "    \n",
    "    # def _generate_prediction(self, input_text: str, max_new_tokens: int = 10, \n",
    "    #                        temperature: int = 1, do_sample: bool = False) -> str:\n",
    "    #     \"\"\"Generate prediction for a single input.\"\"\"\n",
    "    #     inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "    #     outputs = self.model.generate(\n",
    "    #         **inputs,\n",
    "    #         max_new_tokens=max_new_tokens,\n",
    "    #         temperature=temperature,\n",
    "    #         do_sample=do_sample,\n",
    "    #         pad_token_id=self.tokenizer.pad_token_id,\n",
    "    #         eos_token_id=self.tokenizer.eos_token_id\n",
    "    #     )\n",
    "\n",
    "    #     generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    #     prompt_length = len(self.tokenizer.decode(\n",
    "    #         self.tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"][0], \n",
    "    #         skip_special_tokens=True\n",
    "    #     ))\n",
    "        \n",
    "        # return generated_text[prompt_length:].strip()\n",
    "    \n",
    "    def _generate_prediction(self, input_text: str, max_new_tokens: int = 10, \n",
    "                             temperature: float = 1.0, do_sample: bool = False) -> str:\n",
    "        \"\"\"Generate prediction for a single input (returns *only* generated text).\"\"\"\n",
    "        # prepare input tensors\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            # Move tensors to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # safe pad/eos token ids\n",
    "            pad_token_id = getattr(self.tokenizer, \"pad_token_id\", None) or getattr(self.tokenizer, \"eos_token_id\", None)\n",
    "            eos_token_id = getattr(self.tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "\n",
    "            # decode only newly generated tokens (token-level)\n",
    "            input_len = inputs[\"input_ids\"].shape[1]\n",
    "            generated_ids = outputs[0][input_len:]\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            return generated_text.strip()\n",
    "\n",
    "print(\"ModelEvaluator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def evaluate_multiple_datasets(\n",
    "    evaluator,\n",
    "    dataset_paths: List[str],\n",
    "    output_dir: str = \"./multi_dataset_evaluation/\",\n",
    "    split: str = \"test\",\n",
    "    sample_size: Optional[int] = None,\n",
    "    text_column: str = \"text\",\n",
    "    label_column: str = \"label\",\n",
    "    max_new_tokens: int = 10,\n",
    "    system_message: Optional[str] = None,\n",
    "    save_individual_cms: bool = True,\n",
    "    save_summary_report: bool = True\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Evaluate a model on multiple datasets and generate comprehensive reports.\n",
    "    \n",
    "    Args:\n",
    "        evaluator: ModelEvaluator instance\n",
    "        dataset_paths: List of dataset paths (e.g., [\"owner/dataset1\", \"owner/dataset2\"])\n",
    "        output_dir: Directory to save all results\n",
    "        split: Dataset split to use for evaluation\n",
    "        text_column: Name of the text column in datasets\n",
    "        label_column: Name of the label column in datasets\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        system_message: Custom system message (uses default if None)\n",
    "        save_individual_cms: Whether to save individual confusion matrices\n",
    "        save_summary_report: Whether to save summary text report\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Default system message\n",
    "    if system_message is None:\n",
    "        system_message = \"You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\"\n",
    "    \n",
    "    # Store results for all datasets\n",
    "    all_results = {}\n",
    "    summary_lines = []\n",
    "    \n",
    "    # Add header to summary\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    summary_lines.extend([\n",
    "        \"=\"*80,\n",
    "        \"MULTI-DATASET EVALUATION REPORT\",\n",
    "        \"=\"*80,\n",
    "        f\"Evaluation Date: {timestamp}\",\n",
    "        f\"Model Path: {evaluator.model_path}\",\n",
    "        f\"Number of Datasets: {len(dataset_paths)}\",\n",
    "        f\"Split Used: {split}\",\n",
    "        \"=\"*80,\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting evaluation on {len(dataset_paths)} datasets...\")\n",
    "    print(f\"üìÅ Results will be saved to: {output_dir}\")\n",
    "    \n",
    "    # Process each dataset\n",
    "    for idx, dataset_path in enumerate(dataset_paths, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä DATASET {idx}/{len(dataset_paths)}: {dataset_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Load and prepare dataset\n",
    "            start_time = time.time()\n",
    "            dataset_results = _evaluate_single_dataset(\n",
    "                evaluator=evaluator,\n",
    "                dataset_path=dataset_path,\n",
    "                split=split,\n",
    "                text_column=text_column,\n",
    "                label_column=label_column,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                system_message=system_message,\n",
    "                output_dir=output_dir,\n",
    "                save_cm=save_individual_cms,\n",
    "                sample_size=sample_size,\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Store results\n",
    "            dataset_results['evaluation_time_seconds'] = round(end_time - start_time, 2)\n",
    "            all_results[dataset_path] = dataset_results\n",
    "            \n",
    "            # Add to summary\n",
    "            summary_lines.extend(_format_dataset_summary(dataset_path, dataset_results))\n",
    "            \n",
    "            print(f\"‚úÖ Completed {dataset_path} in {dataset_results['evaluation_time_seconds']:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error processing {dataset_path}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            summary_lines.extend([\n",
    "                f\"Dataset: {dataset_path}\",\n",
    "                f\"Status: FAILED\",\n",
    "                f\"Error: {str(e)}\",\n",
    "                \"-\" * 40,\n",
    "                \"\"\n",
    "            ])\n",
    "            all_results[dataset_path] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "    \n",
    "    # Generate comparison visualization\n",
    "    if len([r for r in all_results.values() if r.get('status') != 'failed']) > 1:\n",
    "        _create_comparison_chart(all_results, output_dir)\n",
    "    \n",
    "    # Save summary report\n",
    "    if save_summary_report:\n",
    "        _save_summary_report(summary_lines, all_results, output_dir)\n",
    "    \n",
    "    print(f\"\\nüéâ Evaluation complete! Results saved to: {output_dir}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def _evaluate_single_dataset(\n",
    "    evaluator, dataset_path: str, split: str, text_column: str, \n",
    "    label_column: str, max_new_tokens: int, system_message: str,\n",
    "    output_dir: str, save_cm: bool, sample_size: Optional[int] = None \n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate model on a single dataset.\"\"\"\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"üì• Loading {dataset_path}...\")\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_path)\n",
    "\n",
    "        # # uncomment to use a small sample for testing and debugging purposes\n",
    "        # eval_dataset = dataset[split].select(range(10))\n",
    "        # print(\"Remember to uncomment 'eval_dataset = dataset[split]' to use the full dataset.\")\n",
    "        # # # uncomment to use the full split for evaluation\n",
    "        # # eval_dataset = dataset[split]\n",
    "\n",
    "        # sample_size = None  # or set to e.g. 10 for debugging\n",
    "        if sample_size is not None:\n",
    "            eval_dataset = dataset[split].select(range(min(sample_size, len(dataset[split]))))\n",
    "        else:\n",
    "            eval_dataset = dataset[split]\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(eval_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load dataset: {e}\")\n",
    "    \n",
    "    # Check if columns exist\n",
    "    if text_column not in eval_dataset.column_names:\n",
    "        # Try common alternatives\n",
    "        possible_text_cols = ['text', 'cleaned_text', 'content', 'message', 'tweet']\n",
    "        found_col = None\n",
    "        for col in possible_text_cols:\n",
    "            if col in eval_dataset.column_names:\n",
    "                found_col = col\n",
    "                break\n",
    "        \n",
    "        if found_col:\n",
    "            print(f\"‚ö†Ô∏è  '{text_column}' not found, using '{found_col}' instead\")\n",
    "            text_column = found_col\n",
    "        else:\n",
    "            raise Exception(f\"Text column not found. Available: {eval_dataset.column_names}\")\n",
    "    \n",
    "    if label_column not in eval_dataset.column_names:\n",
    "        possible_label_cols = ['label', 'labels', 'target', 'class', 'category']\n",
    "        found_col = None\n",
    "        for col in possible_label_cols:\n",
    "            if col in eval_dataset.column_names:\n",
    "                found_col = col\n",
    "                break\n",
    "        \n",
    "        if found_col:\n",
    "            print(f\"‚ö†Ô∏è  '{label_column}' not found, using '{found_col}' instead\")\n",
    "            label_column = found_col\n",
    "        else:\n",
    "            raise Exception(f\"Label column not found. Available: {eval_dataset.column_names}\")\n",
    "    \n",
    "    # Convert to ChatML format\n",
    "    print(\"üîÑ Converting to ChatML format...\")\n",
    "    \n",
    "    # def convert_to_chatml(example):\n",
    "    #     text_content = example[text_column]\n",
    "    #     user_prompt = f\"Classify this text: {text_content}\"\n",
    "        \n",
    "    #     # Handle different label formats\n",
    "    #     label = example[label_column]\n",
    "    #     if isinstance(label, (int, float)):\n",
    "    #         label = \"toxic\" if label == 1 else \"normal\"\n",
    "    #     elif isinstance(label, str):\n",
    "    #         # Ensure consistent format\n",
    "    #         label = label.lower().strip()\n",
    "    #         if label in ['1', 'toxic', 'bully', 'cyberbullying', 'hate']:\n",
    "    #             label = \"toxic\"\n",
    "    #         else:\n",
    "    #             label = \"normal\"\n",
    "        \n",
    "    #     return {\n",
    "    #         \"conversations\": [\n",
    "    #             {\"role\": \"system\", \"content\": system_message},\n",
    "    #             {\"role\": \"user\", \"content\": user_prompt},\n",
    "    #             {\"role\": \"assistant\", \"content\": label}\n",
    "    #         ],\n",
    "    #         \"raw_text\": text_content\n",
    "    #     }\n",
    "    \n",
    "    # map: convert to ChatML\n",
    "    def convert_to_chatml(example):\n",
    "        text_content = example.get(text_column, example.get(\"text\", \"\"))\n",
    "\n",
    "        # robust label handling\n",
    "        raw_label = example.get(label_column)\n",
    "        if raw_label is None:\n",
    "            label = \"normal\"\n",
    "        else:\n",
    "            if isinstance(raw_label, (int, float)):\n",
    "                label = \"toxic\" if int(raw_label) == 1 else \"normal\"\n",
    "            else:\n",
    "                normalized = str(raw_label).lower().strip()\n",
    "                # consider numeric strings as well\n",
    "                if normalized in {'1', 'toxic', 'bully', 'cyberbullying', 'hate'} or 'toxic' in normalized:\n",
    "                    label = \"toxic\"\n",
    "                else:\n",
    "                    label = \"normal\"\n",
    "\n",
    "        # user_prompt = f\"Classify this text: {text_content}\"\n",
    "        user_prompt = f\"Classify the text as 'toxic' or 'normal'. Output only one word. Text: '{text_content}'\"\n",
    "        return {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": label}\n",
    "            ],\n",
    "            \"raw_text\": text_content\n",
    "        }\n",
    "\n",
    "\n",
    "    eval_dataset = eval_dataset.map(convert_to_chatml)\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"üß† Running model inference...\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    evaluator.model.eval()\n",
    "\n",
    "    \n",
    "    # Prepare log file\n",
    "    log_file = os.path.join(output_dir, \"invalid_predictions.log\")\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Invalid Predictions Log - Dataset: {dataset_path}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    def log_invalid(sample, true_label, pred_text):\n",
    "        \"\"\"Append invalid predictions to a text file.\"\"\"\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n#\" + \"-\" * 76 + \"#\\n\")\n",
    "            f.write(f\"User prompt: str({sample['conversations'][0]['content']})\\n\")\n",
    "            f.write(f\"True label: str({true_label})\\n\")\n",
    "            f.write(f\"Predicted raw: str({pred_text})\\n\")\n",
    "            f.write(\"#\" + \"-\" * 76 + \"#\")\n",
    "    \n",
    "    #     ## Original code\n",
    "    # for i, sample in enumerate(eval_dataset):\n",
    "    #     if i % 200 == 0:\n",
    "    #         print(f\"  Processed {i}/{len(eval_dataset)} samples ({i/len(eval_dataset)*100:.1f}%)\")\n",
    "        \n",
    "    #     ## Original code\n",
    "    #     input_text = evaluator._format_input(sample[\"conversations\"])\n",
    "    #     pred_text = evaluator._generate_prediction(input_text, max_new_tokens, do_sample=False)\n",
    "        \n",
    "    #     # Convert to binary\n",
    "    #     pred_label = 1 if pred_text.lower() == \"toxic\" else 0\n",
    "    #     true_label = 1 if sample[\"conversations\"][2][\"content\"].lower() == \"toxic\" else 0    \n",
    "    # print(f\"  Processed {len(eval_dataset)}/{len(eval_dataset)} samples (100.0%)\")\n",
    "    \n",
    "    # tqdm will auto-handle progress display\n",
    "    exs = \"\"\n",
    "    for i, sample in enumerate(tqdm(eval_dataset, desc=\"Evaluating\", unit=\"sample\", total=len(eval_dataset))):\n",
    "        try:\n",
    "            input_text = evaluator._format_input(sample[\"conversations\"])\n",
    "            true_text = sample[\"conversations\"][2][\"content\"]\n",
    "            pred_text = evaluator._generate_prediction(input_text, max_new_tokens, do_sample=False)\n",
    "\n",
    "            # üëá Debug: print raw outputs for the first few\n",
    "            if (i < 5) and (true_text == \"toxic\"):\n",
    "                # print(f\"[{i}] INPUT: {input_text.strip()}\")\n",
    "                # print(f\"[{i}] RAW PREDICTION: {pred_text.strip()}\\n\")\n",
    "                exs = exs + 'Input: '+input_text + '\\nPred label: '+pred_text + '\\nTrue label: '+true_text + '\\n'\n",
    "\n",
    "            normalized_pred = pred_text.lower().strip()\n",
    "\n",
    "            # Classify\n",
    "            # if normalized_pred.startswith(\"toxic\"):\n",
    "            #     pred_label = 1\n",
    "            # elif normalized_pred.startswith(\"normal\"):\n",
    "            #     pred_label = 0\n",
    "            # else:\n",
    "            #     # Invalid prediction ‚Üí log and skip\n",
    "            #     log_invalid(input_text, true_text, pred_text)\n",
    "            #     continue  # skip appending to keep predictions/references aligned\n",
    "\n",
    "            if \"toxic\" in normalized_pred.split():\n",
    "                pred_label = 1\n",
    "            elif \"normal\" in normalized_pred.split():\n",
    "                pred_label = 0\n",
    "            else:\n",
    "                log_invalid(input_text, true_label, pred_text)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Ground truth\n",
    "            true_label = 1 if true_text == \"toxic\" else 0\n",
    "\n",
    "            predictions.append(pred_label)\n",
    "            references.append(true_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch runtime errors (generation, formatting, etc.)\n",
    "            with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"-----------------------\\n\")\n",
    "                f.write(f\"Error processing sample: {str(e)}\\n\")\n",
    "                f.write(\"-----------------------\\n\\n\")\n",
    "            continue\n",
    "    print(\"Sample input and pred:\\n\" , exs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(\"üìä Calculating metrics...\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Safeguard against invalid prediction and references\n",
    "    if not predictions:\n",
    "        raise ValueError(\"Predictions list is empty\")\n",
    "    if not references:\n",
    "        raise ValueError(\"References list is empty\")\n",
    "    if len(predictions) != len(references):\n",
    "        raise ValueError(\n",
    "            f\"Length mismatch: predictions={len(predictions)}, references={len(references)}\"\n",
    "        )    \n",
    "    \n",
    "    from collections import Counter\n",
    "    pred_count = Counter(predictions)\n",
    "    print(f\"\\nLabel distribution in predictions: {pred_count}\\n\")\n",
    "\n",
    "    # Calculate the accuracy and f1 scores\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=references)[\"accuracy\"]\n",
    "    # f1 = f1_metric.compute(predictions=predictions, references=references)[\"f1\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")[\"f1\"]\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        references, predictions, \n",
    "        target_names=['normal', 'toxic'], \n",
    "        digits=4,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Save confusion matrix if requested\n",
    "    cm_path = None\n",
    "    if save_cm:\n",
    "        dataset_name = dataset_path.split('/')[-1]  # Get dataset name from path\n",
    "        cm_path = _save_confusion_matrix(references, predictions, output_dir, dataset_name)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"dataset_size\": len(eval_dataset),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix_path\": cm_path,\n",
    "        \"text_column_used\": text_column,\n",
    "        \"label_column_used\": label_column,\n",
    "        \"pred_count\": pred_count\n",
    "    }\n",
    "\n",
    "\n",
    "def _save_confusion_matrix(references: List[int], predictions: List[int], \n",
    "                          output_dir: str, dataset_name: str) -> str:\n",
    "    \"\"\"Save confusion matrix for a dataset.\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(references, predictions)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\", \n",
    "        xticklabels=['normal', 'toxic'], \n",
    "        yticklabels=['normal', 'toxic'], \n",
    "        cbar=False,\n",
    "        square=True,\n",
    "        annot_kws={\"fontsize\": 14}\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=10)\n",
    "    plt.ylabel(\"True Label\", fontsize=10)\n",
    "    plt.title(f\"Gemma3 CM - {dataset_name}\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with dataset name\n",
    "    safe_name = dataset_name.replace('/', '_').replace(' ', '_')\n",
    "    cm_path = os.path.join(output_dir, f\"gemma3_cm_{safe_name}.png\")\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close to avoid memory issues\n",
    "    \n",
    "    return cm_path\n",
    "\n",
    "\n",
    "def _format_dataset_summary(dataset_path: str, results: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Format summary for a single dataset.\"\"\"\n",
    "    \n",
    "    if results.get('status') == 'failed':\n",
    "        return [\n",
    "            f\"Dataset: {dataset_path}\",\n",
    "            f\"Status: FAILED\",\n",
    "            f\"Error: {results.get('error', 'Unknown error')}\",\n",
    "            \"-\" * 40,\n",
    "            \"\"\n",
    "        ]\n",
    "    \n",
    "    lines = [\n",
    "        f\"Dataset: {dataset_path}\",\n",
    "        f\"Samples: {results['dataset_size']:,}\",\n",
    "        f\"Accuracy: {results['accuracy']:.4f}\",\n",
    "        f\"Weighted F1 Score: {results['f1_score']:.4f}\",\n",
    "        f\"Evaluation Time: {results['evaluation_time_seconds']:.1f}s\",\n",
    "    ]\n",
    "    \n",
    "    # Add detailed metrics from classification report\n",
    "    report = results['classification_report']\n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"Detailed Metrics:\",\n",
    "        f\"  Normal - Precision: {report['normal']['precision']:.4f}, Recall: {report['normal']['recall']:.4f}, F1: {report['normal']['f1-score']:.4f}\",\n",
    "        f\"  Toxic  - Precision: {report['toxic']['precision']:.4f}, Recall: {report['toxic']['recall']:.4f}, F1: {report['toxic']['f1-score']:.4f}\",\n",
    "        # f\"  Normal - Precision: {report['0']['precision']:.4f}, Recall: {report['0']['recall']:.4f}, F1: {report['0']['f1-score']:.4f}\",\n",
    "        # f\"  Toxic  - Precision: {report['1']['precision']:.4f}, Recall: {report['1']['recall']:.4f}, F1: {report['1']['f1-score']:.4f}\",\n",
    "        f\"  Macro Avg - Precision: {report['macro avg']['precision']:.4f}, Recall: {report['macro avg']['recall']:.4f}, F1: {report['macro avg']['f1-score']:.4f}\",\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    if results.get('confusion_matrix_path'):\n",
    "        lines.append(f\"Confusion Matrix: {results['confusion_matrix_path']}\")\n",
    "    \n",
    "    lines.extend([\"-\" * 40, \"\"])\n",
    "    return lines\n",
    "\n",
    "\n",
    "def _create_comparison_chart(all_results: Dict[str, Dict], output_dir: str):\n",
    "    \"\"\"Create comparison chart across datasets.\"\"\"\n",
    "    # Extract successful results\n",
    "    successful_results = {k: v for k, v in all_results.items() if v.get('status') == 'success'}\n",
    "    \n",
    "    if len(successful_results) < 2:\n",
    "        return\n",
    "    \n",
    "    dataset_names = [path.split('/')[-1] for path in successful_results.keys()]\n",
    "    accuracies = [results['accuracy'] for results in successful_results.values()]\n",
    "    f1_scores = [results['f1_score'] for results in successful_results.values()]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    bars1 = ax1.bar(dataset_names, accuracies, color='skyblue', alpha=0.8)\n",
    "    ax1.set_title('Gemma3 - Accuracy Comparison Across Datasets', fontsize=14)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    bars2 = ax2.bar(dataset_names, f1_scores, color='lightcoral', alpha=0.8)\n",
    "    ax2.set_title('F1 Score Comparison Across Datasets', fontsize=14)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, f1 in zip(bars2, f1_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{f1:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    comparison_path = os.path.join(output_dir, \"performance_comparison.png\")\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"üìä Comparison chart saved to: {comparison_path}\")\n",
    "\n",
    "\n",
    "def _save_summary_report(summary_lines: List[str], all_results: Dict[str, Dict], output_dir: str):\n",
    "    \"\"\"Save comprehensive summary report to text file.\"\"\"\n",
    "    \n",
    "    # Add overall summary statistics\n",
    "    successful_results = [r for r in all_results.values() if r.get('status') == 'success']\n",
    "    failed_count = len(all_results) - len(successful_results)\n",
    "    \n",
    "    if successful_results:\n",
    "        avg_accuracy = sum(r['accuracy'] for r in successful_results) / len(successful_results)\n",
    "        avg_f1 = sum(r['f1_score'] for r in successful_results) / len(successful_results)\n",
    "        total_samples = sum(r['dataset_size'] for r in successful_results)\n",
    "        \n",
    "        summary_lines.extend([\n",
    "            \"\",\n",
    "            \"OVERALL SUMMARY:\",\n",
    "            f\"‚úÖ Successful evaluations: {len(successful_results)}\",\n",
    "            f\"‚ùå Failed evaluations: {failed_count}\",\n",
    "            f\"üìä Average Accuracy: {avg_accuracy:.4f}\",\n",
    "            f\"üìä Average F1 Score: {avg_f1:.4f}\",\n",
    "            f\"üìà Total samples evaluated: {total_samples:,}\",\n",
    "            \"\",\n",
    "            \"=\"*80,\n",
    "            \"DETAILED RESULTS:\",\n",
    "            \"=\"*80,\n",
    "            \"\"\n",
    "        ])\n",
    "    \n",
    "    # Save to file\n",
    "    report_path = os.path.join(output_dir, \"evaluation_summary.txt\")\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(summary_lines))\n",
    "    \n",
    "    print(f\"üìÑ Summary report saved to: {report_path}\")\n",
    "\n",
    "\n",
    "# Example usage function\n",
    "def run_multi_dataset_evaluation_example():\n",
    "    \"\"\"Example of how to use the multi-dataset evaluation function.\"\"\"\n",
    "    \n",
    "    # Example dataset paths\n",
    "    dataset_paths = []\n",
    "    names = ['stormfront', 'tdavidson', 'olid', 'gab', 'hatexplain']\n",
    "\n",
    "    for name in names:\n",
    "        dataset_paths.append(f\"cike-dev/{name}\")\n",
    "\n",
    "    # model_id = \"cike-dev/GemmaToxicClassifier\"\n",
    "    model_id = \"Gemma3ToxicTextClassifier\"\n",
    "    \n",
    "    # Initialize your evaluator (assuming you have one)\n",
    "    evaluator = ModelEvaluator(model_id)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_multiple_datasets(\n",
    "        evaluator=evaluator,\n",
    "        dataset_paths=dataset_paths,\n",
    "        output_dir=\"./multi_dataset_results/\",\n",
    "        split=\"test\",\n",
    "        sample_size=2000,\n",
    "        text_column=\"text\",  # Adjust as needed\n",
    "        label_column=\"label\"  # Adjust as needed\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "    # print(\"Example function defined. Uncomment and modify the code above to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Gemma3ToxicTextClassifier...\n",
      "==((====))==  Unsloth 2025.9.4: Fast Gemma3_Text patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.278 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded successfully!\n",
      "\n",
      "üöÄ Starting evaluation on 5 datasets...\n",
      "üìÅ Results will be saved to: ./multi_dataset_results/\n",
      "\n",
      "============================================================\n",
      "üìä DATASET 1/5: cike-dev/stormfront\n",
      "============================================================\n",
      "üì• Loading cike-dev/stormfront...\n",
      "‚úÖ Loaded 2000 samples\n",
      "‚ö†Ô∏è  'text' not found, using 'cleaned_text' instead\n",
      "üîÑ Converting to ChatML format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5254b28c8e394ef6975fa328cf8fc456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Running model inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [06:34<00:00,  5.07sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input and pred:\n",
      " Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'the negro is generally terrified of whites , the second you stand up to them they usally back away , thats unless they re carrying a gun at which point the savages are rarely reluctant to use .'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: toxic\n",
      "True label: toxic\n",
      "\n",
      "üìä Calculating metrics...\n",
      "\n",
      "Label distribution in predictions: Counter({0: 1232, 1: 603})\n",
      "\n",
      "‚úÖ Completed cike-dev/stormfront in 396.9s\n",
      "\n",
      "============================================================\n",
      "üìä DATASET 2/5: cike-dev/tdavidson\n",
      "============================================================\n",
      "üì• Loading cike-dev/tdavidson...\n",
      "‚úÖ Loaded 2000 samples\n",
      "‚ö†Ô∏è  'text' not found, using 'cleaned_text' instead\n",
      "üîÑ Converting to ChatML format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b9c16b274d4f3f98c9d42f5ab60d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Running model inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [07:52<00:00,  4.23sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input and pred:\n",
      " Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'im happy for the bung holes and negativity desperate guys spew on here ;a good example of what not to be and what i don;'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: normal\n",
      "True label: toxic\n",
      "Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'smh u niggers and ur music knowledge'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: toxic\n",
      "True label: toxic\n",
      "\n",
      "üìä Calculating metrics...\n",
      "\n",
      "Label distribution in predictions: Counter({1: 1030, 0: 687})\n",
      "\n",
      "‚úÖ Completed cike-dev/tdavidson in 474.6s\n",
      "\n",
      "============================================================\n",
      "üìä DATASET 3/5: cike-dev/olid\n",
      "============================================================\n",
      "üì• Loading cike-dev/olid...\n",
      "‚úÖ Loaded 2000 samples\n",
      "‚ö†Ô∏è  'text' not found, using 'cleaned_text' instead\n",
      "üîÑ Converting to ChatML format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a7af1dafb647b0aede9ec768daa8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Running model inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [11:54<00:00,  2.80sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input and pred:\n",
      " Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'it makes my skin crawl just looking at this woman  theresa may is a liability to our nation and  and must be removed asap'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: normal\n",
      "True label: toxic\n",
      "Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'how? you cant just say wrong with no evidence for why it is. you said you agree with sargon im pretty much every subject but gun control and healthcare. thats not what a centrist believes.'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: normal. gun control and healthcare are the most common\n",
      "True label: toxic\n",
      "\n",
      "üìä Calculating metrics...\n",
      "\n",
      "Label distribution in predictions: Counter({0: 737, 1: 438})\n",
      "\n",
      "‚úÖ Completed cike-dev/olid in 715.8s\n",
      "\n",
      "============================================================\n",
      "üìä DATASET 4/5: cike-dev/gab\n",
      "============================================================\n",
      "üì• Loading cike-dev/gab...\n",
      "‚úÖ Loaded 2000 samples\n",
      "‚ö†Ô∏è  'text' not found, using 'cleaned_text' instead\n",
      "üîÑ Converting to ChatML format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b288c0c7a844d490c1cab68912fb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Running model inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [12:19<00:00,  2.70sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input and pred:\n",
      " Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'two bits of feedback. first, that catholic church shill that calls in is getting annoying as fuck. second, when people ask you to let jews talk when they call in, tell them that they can turn on any msm if they are so desperate to listen to jews. your show is an oasis of jew free content, please dont let them speak there.'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: normal. the jews are not going to listen\n",
      "True label: toxic\n",
      "Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'international womens day today eh, hope the ladies are doing their bit to level up the 95 of workplace deaths and 66 suicide rate. i worry for boys growing up now, told they are toxic, brought up with few male role modelsteachers and assumed to be potential rapists.¬† never mind, we are disposable.'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: normal'\n",
      "True label: toxic\n",
      "\n",
      "üìä Calculating metrics...\n",
      "\n",
      "Label distribution in predictions: Counter({0: 716, 1: 531})\n",
      "\n",
      "‚úÖ Completed cike-dev/gab in 741.4s\n",
      "\n",
      "============================================================\n",
      "üìä DATASET 5/5: cike-dev/hatexplain\n",
      "============================================================\n",
      "üì• Loading cike-dev/hatexplain...\n",
      "‚úÖ Loaded 2000 samples\n",
      "‚ö†Ô∏è  'text' not found, using 'cleaned_text' instead\n",
      "üîÑ Converting to ChatML format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c69ba32158456b87acf32ba39b885e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Running model inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [05:52<00:00,  5.67sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input and pred:\n",
      " Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'low iq mad maxine living up to her dumb nigger name'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: toxic\n",
      "True label: toxic\n",
      "Input: <start_of_turn>user\n",
      "You are a binary classifier for cyberbullying speech. Respond only with 'toxic' or 'normal'.\n",
      "\n",
      "Classify the text as 'toxic' or 'normal'. Output only one word. Text: 'another brain fart from morbious stone no actually you just displayed you are some king of retard boomer who can not read rotfl stop sucking alex jewns dick must have just graduated from kinder garden lowest iq list lmao'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Pred label: toxic\n",
      "True label: toxic\n",
      "\n",
      "üìä Calculating metrics...\n",
      "\n",
      "Label distribution in predictions: Counter({1: 1256, 0: 645})\n",
      "\n",
      "‚úÖ Completed cike-dev/hatexplain in 354.8s\n",
      "üìä Comparison chart saved to: ./multi_dataset_results/performance_comparison.png\n",
      "üìÑ Summary report saved to: ./multi_dataset_results/evaluation_summary.txt\n",
      "\n",
      "üéâ Evaluation complete! Results saved to: ./multi_dataset_results/\n"
     ]
    }
   ],
   "source": [
    "evval = run_multi_dataset_evaluation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
